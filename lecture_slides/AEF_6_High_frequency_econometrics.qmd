---
title: "Advanced Empirical Finance: Topics and Data Science"
author: "Stefan Voigt"
institute: "University of Copenhagen and Danish Finance Institute"
date: today
date-format: "[Spring] YYYY"
format: beamer
pdf-engine: lualatex

theme: metropolis
fontsize: 10pt
mainfont: Fira Sans
monofont: Source Code Pro
monofontoptions: 
  - Scale=0.9
mathfont: firamath

knitr:
  opts_chunk: 
    size: "tiny"
    fig.width: 8
    fig.height: 4
    fig.align: "center"
    out.width: "60%"
    R.options:
      tibble.print_max : 4
      tibble.print_min : 4
      formatR.indent: 2
      digits : 3
      crayon.enabled: FALSE

execute:
  echo: FALSE
  cache: FALSE
  eval: TRUE
  message: FALSE
  warning: FALSE

header-includes: |-
  \metroset{progressbar=frametitle} 
  \usepackage{graphicx} 
  \usepackage{booktabs} 
  \usepackage{amsmath,amsfonts,amssymb} 
  \usepackage{listings} 
  \setbeamercolor{progress bar}{fg=red} 
  \setbeamercolor{frametitle}{fg=black,bg=white} 
  \setbeamercolor{background canvas}{bg=white} 
  \usepackage{hyperref} 
  \hypersetup{colorlinks=false}
  \setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
---

```{r}
#| include: false
source("pre_session_script.R")
```

# High-frequency econometrics

## High-frequency trading (?)

![]("./figures/tuchmann_broker.jpg"){width=80% fig-align="center"}


## High-frequency trading

- So far, we have focused on quarterly, monthly, or daily price observations
- But: There is much more under the surface!
- High-frequency trading is a type of algorithmic financial trading characterized by high speeds, high turnover rates, and high order-to-trade ratios that leverages high-frequency financial data and electronic trading tools

### Relevance for asset pricing

1. liquidity (transaction costs) 
2. price informativeness (have you asked yourself how information makes it into prices?) (see also KU's market microstructure course)
3. more information may be beneficial for volatility estimation
4. wasteful investments?

## Quick primer on equity microstructure

- Market structure across asset classes can vary tremendously
- Large heterogeneity across US equity markets (lit pools, dark pools, maker-taker, ...)
- Standard framework: Continuous limit order book 
- Limit orders indicate a willingness to buy/sell at a pre-specified price
- Market orders execute against limit orders

![]("./figures/orderbook.jpg"){width=60% fig-align="center"}

##  A typical NASDAQ trading day

- **Opening auction** at 9:30 am
- Continuous trading and order book adjustments throughout the entire day
- Message types: Cancellations/Submissions/Executions/ Hidden or Lit orders

![]("./figures/orderbook_dynamic.png"){width=80% fig-align="center"}


- The number of daily messages is easily in the millions
- **Closing auction** at 4:00 pm 

# Realized volatility

## HFT: Approaching continuous time finance

- Does it help to use data sampled at a high frequency to estimate $\mu_t$ and $\sigma_t^2$?
- Continuous-time finance helps
- Well-known example: Geometric Brownian motion (stock price movements are additive on the log scale)
$$\log(P_t) = X_t = X_0 + \mu t + \sigma W_t$$
where $W_t$ is a Brownian motion

### Brownian motion

- The process $(W_t)_{0\leq t\leq T}$ is a Brownian motion provided that
  1. $W_0 = 0$
  2. $t \rightarrow W_t$ is a continuous function of $t$
  3. $W$ has independent increments and for $t>s$, $W_t - W_s$ is normal with mean zero and variance $t-s$
  
## Brownian motion
- Suppose $t = 0$ is the start of the training day and $t=1$ is the end of the day
- Assume there are $n$ equidistant observations (transactions) of the log  price
- One observation every $\Delta_{t_n} = 1/n$ units of time
- We observe $X_{t_{n,i}}$ with $t_{n,i} = i\Delta t_n$ and get 
$$\Delta X_{t_{n,i}} = X_{t_{n,i}} - X_{t_{n,i -1}} $$
- By definition of the BM, the $\Delta X_{t_{n,i}}$ are iid normal $N(\mu\Delta t_n, \sigma^2 \Delta t_n)$

## Simulation of a Brownian motion

```{r}
simulate_brownian_motion <- function(seed = 1, steps = 1000, sigma = 1, mu = 1.3, start = 100){
  set.seed(seed)
  date <- Sys.Date() + days(seed)
  hour(date) <- 9
  minute(date) <- 30
  date_end <- date
  hour(date_end) <- 16
  minute(date_end) <- 0
  tibble(step = 1:(steps+1), time =   seq(from = date, to = date_end, length.out = steps + 1),
         price = cumsum(c(start, rnorm(steps, mean= mu/steps, sd = sigma/sqrt(steps)))))
}
simulation <- tibble(iteration = 1:400) |>
  mutate(values = map(iteration, simulate_brownian_motion, start = 1)) |> 
  unnest(values)

p <- simulation |> ggplot(aes(x = step, y = price - 1, group = iteration)) + geom_line(alpha = 0.3) +
    labs(x = "Day", y = "cum. return") + geom_point(data = simulation |> filter(step == max(step)), size = 0.1) 
ggExtra::ggMarginal(p, margins = 'y', color = "purple")
```

## Estimating $\mu$ and $\sigma^2$ in the GBM model

- The natural estimator for $\hat\mu$ is $$\hat\mu_n = \frac{1}{n\Delta t_n}\sum\limits_{i = 0}^{n-1}\Delta X_{t_n, i+1} = (X_1 - X_0)$$
- (Maybe) surprising: $\hat\mu_n$ does not depend on the sampling frequency. Consistent estimation requires $t\rightarrow \infty$
- (Maybe) more surprising: $\hat\sigma^2$ *can* be estimated consistently as $n\rightarrow \infty$!
- Set $U_{n,i} = \Delta X_{t_n,i}/(\sigma\Delta t_n^{1/2}) \sim N\left(\frac{\mu}{\sigma}\Delta t_n^{1/2}, 1\right)$ and define $\bar U_n = \frac{1}{n}\sum\limits_{i = 0}^{n-1}\left(U_{n,i} - \bar U_n\right)^2$
- Then $$\begin{aligned}\hat\sigma_n^2 &= \frac{1}{(n-1)\Delta t_n}\sum\limits_{i = 0}^{n-1}\left(\Delta X_{t_n, i+1} - \bar{\Delta X_{t_n}}\right)^2 \\&= \frac{\sigma^2\Delta t_n}{(n-1)\Delta t_n}\sum\limits_{i = 0}^{n-1}\left(\Delta U_{n, i} - \bar{U_{n}}\right)^2 \stackrel{\mathcal{L}}{=} \sigma^2\frac{\chi^2_{n-1}}{n-1}\end{aligned}$$
- It follows that $E\left(\hat\sigma_n^2\right) = \sigma^2$ and $\text{Var}\left(\hat\sigma_n^2\right) = \frac{2\sigma^4}{n-1}$

## Non-centered estimator

- for high frequency data, the mean $\bar{\Delta X_{t_n}}$ is often **not** removed in estimation
- Instead, consider $$\hat\sigma^2_{n,\text{nocenter}} = \frac{1}{n\Delta t_n}\sum\limits_{i=0}^{n-1}\left(\Delta X_{t_{n, i +1}}\right)^2 = \hat\sigma_n^2 + \Delta t_n\bar\mu_n^2 = \hat\sigma_n^2 + \frac{1}{n}\bar\mu_n^2 $$
- Since $\bar\mu_n$ does not depend on $n$, it follows that $\hat\sigma^2_{n,\text{nocenter}}$ is also consistent

## The realized volatility

- If $\mu_t$ and $\sigma_t>0$ are predictable processes of finite variation and  we assume the diffusion $$dX_t = \mu_tdt + \sigma_t dW_t$$ the continuously compounded return over the time interval from $t-k$ to $t$ is 
$$r(t,k) = X_t - X_{t-k} = \int\limits_{t-k}^t \mu_t dt + \int\limits_{t-k}^t \sigma_t dW_t$$
- The diffusive sample path variation is called the integrated variance
$$IV(t-k, k) = \int\limits_{t-k}^t \sigma_t^2 dt $$
- As long as there are no jumps, $IV(0, 1) = RV^{(n)} := \sum\limits_{i=0}^{n-1}r_{j,n}^2$
- $RV^{(n)}$ is the **realized variance** 
- 'Infill' asymptotics: Sampling on the highest possible frequencies crucial!

## Infill asymptotics

![]("./figures/infill_asymptotics.jpg"){width=80% fig-align="center"}

## S&P 500 - Realized Volatility from 1988
```{r}
data <- read_csv("../data/SPX.txt") |> 
  transmute(ts = as.POSIXct(paste(Date, Time), tz  ="GMT", format = "%m/%d/%Y %H%M"),
            price = as.numeric(Close)) |>
  group_by(date = as.Date(ts)) |> 
  mutate(return = (log(price) - lag(log(price))))
data |> summarise(last_price = last(price),
            "Realized Volatility" = 100 * sum(return ^ 2, na.rm = TRUE)) |>
  mutate("Squared daily return" = (100 * (log(last_price) - lag(log(last_price))) ^2)) |> 
  pivot_longer(-c(date:last_price)) |> 
  ggplot(aes(x = date, y = value, color = name)) + geom_point(size = 0.1, alpha = 0.3) +
  scale_y_sqrt() + labs(x = "", y = "Volatility", color = NULL) + theme(legend.position = "bottom") + 
  scale_x_date(breaks = function(x) seq.Date(from = min(x), to = max(x), by = "1 years"), labels = scales::date_format("%y"))
```

# Market Microstructure Noise

## Market Microstructure Noise

- Problem in practice: market microstructure frictions
- E.g., Bid-ask spreads, price discreteness, asymmetric information, strategic order placement
- We can only observe
$$Y_{i\Delta n} = X_{i\Delta n} + U _{i\Delta n}$$
where $Y_{i\Delta n}$ is the observed (log) transaction price or quote, $X_{i\Delta n}$ is the *efficient* latent (log) price
- $U_{i\Delta n}$ is white noise which captures microstructure frictions

## What is the problem with microstructure noise? 

![]("./figures/aitsahalia_1.jpg"){width=90% fig-align="center"}


- So far: the highest possible sampling frequency is optimal

## What is the problem with microstructure noise? 

![]("./figures/aitsahalia_2.jpg"){width=90% fig-align="center"}

- Now: What is the RV estimator's optimal sampling frequency/adjustment in the presence of microstructure noise?

## What is the problem with microstructure noise? 

- Consider for now the easiest case with $X_t = \sigma W_t$ and $\tilde X_t = X_t + U_t$ where the $U_t$ are iid noise with zero mean and variance $a^2$
- Then
$$\begin{aligned}Y_{\tau_i} &= \tilde X_{\tau_i} - \tilde X_{\tau_{i-1}} \\&= \left(X_{\tau_i} -  X_{\tau_{i-1}}\right) + U_{\tau_i} - U_{\tau_{i-1}} \\&= \sigma \left(W_{\tau_i} -  W_{\tau_{i-1}}\right) + U_{\tau_i} - U_{\tau_{i-1}}\end{aligned}$$
- We get $\text{Var}\left(Y_{\tau_i}\right) = \sigma^2\Delta_n + 2\alpha^2$ and $\text{Cov}\left(Y_{\tau_i}, Y_{\tau_{i-1}}\right) = -\alpha^2$
- the proportion of the total return variance that is market
microstructure-induced is $$\pi = \frac{2\alpha^2}{2\alpha^2 + \sigma^2\Delta_n}$$
- As $\Delta_n$ gets small, a larger portion of the volatility reflects noise!
- Noise bias adjustment possible (Ait-Sahalia et al., 2005)


## Optimal sampling frequencies (Ait-Sahalia et al., 2005)

![]("./figures/aitsahalia_4.jpg"){width=90% fig-align="center"}

-  the presence of market microstructure noise makes it optimal to sample less often than would otherwise be the case in the absence of noise
