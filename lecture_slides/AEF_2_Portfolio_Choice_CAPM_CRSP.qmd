---
title: "Advanced Empirical Finance: Topics and Data Science"
author: "Stefan Voigt"
institute: "University of Copenhagen and Danish Finance Institute"
date: today
date-format: "[Spring] YYYY"
format: beamer
pdf-engine: lualatex

theme: metropolis
fontsize: 10pt
mainfont: Fira Sans
monofont: Source Code Pro
monofontoptions: 
  - Scale=0.9
mathfont: firamath

knitr:
  opts_chunk: 
    size: "tiny"
    fig.width: 8
    fig.height: 4
    fig.align: "center"
    out.width: "60%"
    R.options:
      tibble.print_max : 4
      tibble.print_min : 4
      formatR.indent: 2
      digits : 3
      crayon.enabled: FALSE

execute:
  echo: TRUE
  cache: FALSE
  eval: TRUE
  message: FALSE
  warning: FALSE

header-includes: |-
  \metroset{progressbar=frametitle} 
  \usepackage{graphicx} 
  \usepackage{booktabs} 
  \usepackage{amsmath,amsfonts,amssymb} 
  \usepackage{listings} 
  \setbeamercolor{progress bar}{fg=red} 
  \setbeamercolor{frametitle}{fg=black,bg=white} 
  \setbeamercolor{background canvas}{bg=white} 
  \usepackage{hyperref} 
  \hypersetup{colorlinks=false}
  \setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
---

```{r}
#| include: false
source("pre_session_script.R")
```

```{python}
#| include: false
from plotnine import theme_set, theme, theme_bw
theme_set(theme_bw() + theme(legend_position="bottom"))
```

# Asset prices and returns

## Asset prices and returns

![](figures/rollercoaster.jpg){width="60%" fig-align="center"}

**Topics** 

- Stock market returns
- Diversification and risk management

## General framework and notation

- We consider $N$ assets, time series of prices $P_{t,i}$ of asset $i$ at time $t$
- Net returns are $r_{t+1, i} := (P_{t+1, i} - P_{t, i})/P_{t, i}$ 
- Gross returns are $R_{t+1, i} := P_{t+1, i}/P_{t, i} =  1 + r_{t+1, i}$
- I refer to log returns as $\tilde r_{t+1, i} := \log(P_{t+1, i}) - \log(P_{t, i})$
- Dividends: modify the definitions, e.g., $\tilde r_{t+1} = \log(P_{t+1, i} + D_{t+1,i }) - \log(P_{t, i})$

```{r}
library(tidyverse)
library(tidyquant)
prices_sp500 <- tq_get("^GSPC", from = "2000-01-01") |> select(date, adjusted)
prices_sp500 <- prices_sp500 |> mutate(net_return = (adjusted - lag(adjusted)) / lag(adjusted),
                                       gross_return = adjusted / lag(adjusted),
                                       log_return = log(adjusted) - log(lag(adjusted)))  
head(prices_sp500)
prices_sp500 <- prices_sp500 |> drop_na() # Remove NAs
```

## Distributional properties of returns

### Moments of the return distribution

- Define $\mu = E(r_{t+1})$ and $\Sigma = \text{Cov}(r_{t+1}) = E((r_{t+1} - \mu)(r_{t+1} - \mu)')$
- **Later:** $\mu_{\textcolor{red}{t}} := E(r_{t+1}|\mathcal{F}_t)$ and $\Sigma_{\textcolor{red}{t}} = \text{Cov}(r_{t+1}|\mathcal{F}_t)$ where $\mathcal{F}_t$ denotes the available information at $t$

### Sample moments
- Suppose you have $T$ observations of the $(N\times1)$ vector, $r_1, \ldots, r_{t}, \ldots, r_T$
- The sample counterparts $\hat{\mu}$ and $\hat{\Sigma}$ are
$$
\hat\mu = \frac{1}{T}\sum\limits_{t=1}^Tr_t \text{ and } \hat{\Sigma} = \frac{1}{T-1}\sum\limits_{t=1}^T\left((r_t - \hat{\mu})(r_t - \hat{\mu})'\right)
$$

```{r}
returns <- prices_sp500 |> pull(net_return)
c(sum(returns)/length(returns), mean(returns)) # (daily) average
c(sum((returns - mean(returns))^2)/(length(returns) - 1), var(returns)) # (daily) variance
```

## Distributional properties of returns

- Estimate the sample variance-covariance matrix $\hat\Sigma$

```{r}
prices <- tq_get(c("META", "AAPL", "MSFT"), from = "2000-01-01")
price_matrix <- prices |> 
  group_by(symbol) |> # Net returns per symbol
  transmute(date, return = (adjusted - lag(adjusted)) / lag(adjusted)) |>  
  pivot_wider(names_from = symbol, values_from = return) |> 
  drop_na() |> select(-date) 
(sigma <- price_matrix |> cov())
```

- Volatility $\sigma$ is the standard deviation (the square root of the variance $\sigma^2$)    
- For daily data, the annualized volatility and mean are $\approx\sqrt{250}\hat\sigma$ and $\approx250\hat\mu$
```{r}
bind_rows(100 * sqrt(250 * diag(sigma)), # Annualized volatility (in percent),
          100 * 250 * colMeans(price_matrix)) # Annualized return (in percent))
```

# Optimal portfolio allocation

## Optimal (static) portfolio choice
- **Aim:** Choose $(N \times 1)$ vector $\omega$ such that $\sum\limits_{t=1}^T \omega_i = \iota'\omega = 1$ where $\iota$ is an $(N\times 1)$ vector of ones
- Portfolio returns $r^{pf}_{t} = \omega'r_t$
- Common properties of utility functions (concave) 
$$U'(r_t) > 0 \text{ and } U(E(r_t)) > E(U(r_t))$$ 
- Preference for higher expected return and lower volatility $\sigma^{pf} =\sqrt{\text{Var}\left(r^{pf}_t\right)}$
$$\begin{aligned}
E(r^{pf}_{t}) = \omega'\mu \qquad (\sigma^{pf})^2 &= E\left(\omega'(r_t - \mu)(r_t- \mu)'\omega\right) \\ &= E\left(tr\left(\omega'(r_t - \mu)(r_t- \mu)'\omega\right)\right)
\\ &= tr\left(\omega'E\left((r_t - \mu)(r_t- \mu)\right)'\omega\right) = \omega'\Sigma \omega
\end{aligned}$$

## Minimum variance portfolio
- The minimum variance portfolio weights are given by the solution to 
$$\omega_\text{mvp} = \arg\min \omega'\Sigma \omega \text{ s.t. } \iota'\omega= 1$$
- The Lagrangian reads 
$$ \mathcal{L}(\omega) = \omega'\Sigma \omega - \lambda(\omega'\iota - 1)$$
- Analytic solution by solving the first-order conditions of the Lagrangian equation
$$
\begin{aligned}
& \frac{\partial\mathcal{L}(\omega)}{\partial\omega} = 0 \Leftrightarrow 2\Sigma \omega = \lambda\iota \Rightarrow \omega = \frac{\lambda}{2}\Sigma^{-1}\iota \\ \end{aligned}
$$
- Constraint delivers: $1 = \iota'\omega = \frac{\lambda}{2}\iota'\Sigma^{-1}\iota \Rightarrow \lambda = \frac{2}{\iota'\Sigma^{-1}\iota}$
$$
\begin{aligned}
\Rightarrow &\omega_\text{mvp} = \frac{\Sigma^{-1}\iota}{\iota'\Sigma^{-1}\iota}
\end{aligned}
$$

- Variance of the minimum variance portfolio return is $\omega_\text{mvp}'\Sigma \omega_\text{mvp} = \frac{1}{\iota'\Sigma^{-1}\iota}$

## Computing the minimum variance portfolio with R
- Suppose you know the variance-covariance matrix, e.g., 
$$\Sigma = \begin{pmatrix} 3 & 2 \\ 2 & 4\end{pmatrix} \text{ with }\Sigma^{-1} = \frac{1}{8}\begin{pmatrix}4 & -2\\-2 &3\end{pmatrix}$$ 
- Then you can compute the minimum variance portfolio weights as follows

```{r}
Sigma <- matrix(c(3, 2, 2, 4), ncol = 2) # Define 2 x 2 matrix Sigma
Sigma_inv <- solve(Sigma) # Invert the matrix
w <- Sigma_inv %*% rep(1, 2) # %*% is matrix multiplication in R
w <- w / sum(w)

t(w) # t() transposes a vector/matrix
```

- The minimum variance portfolio weights are thus $\omega_\text{mvp} = \frac{1}{3}\begin{pmatrix}2 & 1\end{pmatrix}$
- \textcolor{red}{Which practical issues are important here?}

## The efficient portfolio
- Consider an investor who aims to achieve minimum variance *given a desired expected return* $\bar{\mu}$
$$\omega_\text{eff}\left(\bar{\mu}\right) = \arg\min \omega'\Sigma \omega \text{ s.t. } \iota'\omega = 1 \text{ and } \omega'\mu \geq \bar{\mu}$$
- The Lagrangian reads 
$$ \mathcal{L}(\omega) = \omega'\Sigma \omega - \lambda(\omega'\iota - 1) - \tilde{\lambda}(\omega'\mu - \bar{\mu}) $$
- Solve the first-order conditions
$$
\begin{aligned}
2\Sigma \omega &= \lambda\iota + \tilde\lambda \mu\\
\omega &= \frac{\lambda}{2}\Sigma^{-1}\iota + \frac{\tilde\lambda}{2}\Sigma^{-1}\mu
\end{aligned}
$$

## The efficient portfolio
- The two constraints ($\omega'\iota = 1 \text{ and } \omega'\mu \geq \bar{\mu}$) imply 
$$
\begin{aligned}
1 &= \iota'\omega = \frac{\lambda}{2}\underbrace{\iota'\Sigma^{-1}\iota}_{C} + \frac{\tilde\lambda}{2}\underbrace{\iota'\Sigma^{-1}\mu}_D
\Rightarrow \lambda = \frac{2 - \tilde\lambda D}{C}\\
\bar\mu &= \mu'\omega = \frac{\lambda}{2}\underbrace{\mu'\Sigma^{-1}\iota}_{D} + \frac{\tilde\lambda}{2}\underbrace{\mu'\Sigma^{-1}\mu}_E = \frac{1}{2}\left(\frac{2 - \tilde\lambda D}{C}\right)D+\frac{\tilde\lambda}{2}E  \\&=\frac{D}{C}+\frac{\tilde\lambda}{2}\left(E - \frac{D^2}{C}\right)\\
\Rightarrow \tilde\lambda &= 2\frac{\bar\mu - D/C}{E-D^2/C}\\
\end{aligned}
$$
- As a result, the efficient portfolio weight takes the form (for $\bar{\mu} \geq D/C = \mu'\omega_\text{mvp}$)
$$\omega_\text{eff}\left(\bar\mu\right) = \omega_\text{mvp} + \frac{\tilde\lambda}{2}\left(\Sigma^{-1}\mu -\frac{D}{C}\Sigma^{-1}\iota \right)$$

## The efficient portfolio
- Note that $$\iota'\left(\Sigma^{-1}\mu -\frac{D}{C}\Sigma^{-1}\iota \right) = D - D = 0\text{ so }\iota'\omega_\text{eff} = \iota'\omega_\text{mvp} = 1$$ and $$\mu'\omega_\text{eff} = \frac{D}{C} + \bar\mu - \frac{D}{C} = \bar\mu$$
- The efficient portfolio allocates wealth in the minimum variance portfolio $\omega_\text{mvp}$ and a levered (self-financing) portfolio to increase the expected return

### Two mutual fund theorem
- Assume you computed $\omega_\text{eff}(\bar\mu)$ and $\omega_\text{eff}(\tilde\mu)$ for $\bar\mu > \tilde \mu \geq D/C$, then any linear combination with $c\in\mathbb{R}_+$ can be represented as  
$$\omega^* = c\omega_\text{eff}(\bar\mu) + (1-c)\omega_\text{eff}(\tilde\mu) = \omega_\text{mvp} + \frac{\lambda^*}{2}\left(\Sigma^{-1}\mu -\frac{D}{C}\Sigma^{-1}\iota \right)$$ with $\lambda^* = 2\frac{c\bar\mu + (1-c)\tilde\mu - D/C}{E-D^2/C}$.
- As a result, any portfolio of two efficient portfolios is also efficient

## The efficient frontier (coding exercise)
- Download historical price data for *all* stocks that are part of the Dow Jones index and obtain estimates $\hat{\mu}$ and $\hat{\Sigma}$.
- Compute $\omega_\text{mvp}$ and arbitrary $\omega_\text{eff}(\bar{\mu})$
- Then, apply the two-fund theorem to characterize the *entire* efficient frontier

![](figures/efficient_frontier.png){width=60% fig-align="center"}

## The efficient frontier with a risk-free rate

- Assume there is a $N+1$-th asset which offers a risk-free rate $r_f > 0$ and has zero volatility
- The investor allocates fractions $x$ of wealth to the risky assets and the remainder $(1 - \iota'x)$ to the risk-free asset with portfolio return
$$r_t^\text{pf} = x'r_t + (1-\iota'x)r_f = r_f + x'(r_t - r_f\iota)$$
- The mean-variance problem can be expressed as 
$$\min x'\Sigma x \text{ s.t. } x'(\mu-r_f) \geq \bar\mu$$
- The solution is much simpler than before: 
$$x^* = \frac{\bar\mu}{(\mu-r_f)'\Sigma^{-1}(\mu-r_f)} \Sigma^{-1}(\mu-r_f)$$
- The resulting portfolio of investments in risky assets, $\frac{x^*}{\iota'x^*}$ is the **efficient tangent portfolio**
$$\omega_\text{tgc} = \frac{\Sigma^{-1}(\mu-r_f)}{\iota'\Sigma^{-1}(\mu-r_f)}$$

- \textcolor{red}{Why does $\bar\mu$ not show up in $\omega_{tgc}$?} 

## Empirical problems (Discussion)

Let's move from theory to practice: 

1. Which decisions do you as a portfolio manager have to take?
1. What issues may occur and lead to deviations from the theoretically optimal portfolio?
1. How do you evaluate your choices?

<!-- - What is $\mu$? What is $\Sigma$? -->
<!-- - Real-world constraints? Transaction costs, taxes, short-selling -->
<!-- - Static framework versus dynamic -->
<!-- - Appropriate asset universe? -->
<!-- - Non-traded assets (human capital, VC) also exist  -->

## Portfolio backtesting

- Portfolio backtesting is often perceived as a quest to find the best strategy or at least a solidly profitable one (downside: data snooping, **p**-hacking)
- Out-of-sample test to analyze the (hypothetical) performance of a strategy

### Procedure

1. Fix investment horizon $N$, sample period $T$ and estimation window size $h$
1. Out-of-sample: Never use information an investor would not have at the time of decision
1. For each period, recompute $\hat\Sigma$ and $\hat\mu$ and reallocate wealth
1. At the end of each period store the portfolio performance (e.g. return)
1. Compare different strategies by evaluating the average out-of-sample performance

```{r}
#| echo: false
knitr::include_graphics("figures/backtestoos.png")
```

## Evaluation metrics

- Typical metrics are the out-of-sample portfolio return (eventually risk-adjusted)
$$\hat E(r^{pf}) = \frac{1}{T-h-1}\sum\limits_{t=h+1}^T r^{pf}_t$$
Note that $r_t^{pf} = \omega_t'r_{t+1}$ where $\omega_t$ denotes the weights based on information available up to time $t$
- Portfolio volatility $\sqrt{\hat\sigma^2\left(r^{pf}\right)}$
- Later we will also consider transaction costs which depend on rebalancing from $\omega_{t-1}$ to $\omega_{t}$

## Rolling windows in R and Python

- You will need `for` loops to mimic sliding through time
- `for` loops provide a way to tell, “Do *this* for every value of *that*.” In R and Python syntax, this looks like this:

:::: {.columns}
::: {.column width="45%"}
```{r}
# R
for (value in c("My", "first", "for", "loop")) {
  print(value)
}
```
:::
::: {.column width="45%"}
```{python}
# Python
for value in ["My", "first", "for", "loop"]:
    print(value)
```
:::
::::

- \textcolor{red}{Task: Develop pseudo-code for portfolio backtesting}

## Functions in R and Python

- Writing a function has three big advantages over using copy-and-paste:

1. Functions with an evocative name make your code easier to understand
2. As requirements change, you only need to update the code in one place
3. You eliminate the chance of making incidental mistakes when you copy and paste

- Keep in mind: functions are for computers but also *for humans* to make clear code 
- Use names that explain what the function is doing and make use of comments `#` for readers (including future-you)

:::: {.columns}
::: {.column width="45%"}
```{r}
# R
compute_sum <- function(x, y){
  return(x+y)
}
```
:::
::: {.column width="45%"}
```{python}
# Python
def compute_sum(x, y):
    return x + y
```
:::
::::

## 3 key steps to creating a new function

- Pick a meaningful name for the function 
- List the inputs to the function inside `function(...)` 
- Place code in the body of the function, a `{ }` block that immediately follows `function(...)`

```{r}
compute_efficient_portfolio <- function(sigma, mu, mu_bar = 0.30 / 250){
  iota <- rep(1, ncol(sigma))
  sigma_inv <- solve(sigma) 
  w_mvp <- sigma_inv %*% iota
  w_mvp <- w_mvp / sum(w_mvp)
  C <- as.numeric(t(iota)%*%sigma_inv%*%iota)
  D <- as.numeric(t(iota)%*%sigma_inv%*%mu)
  E <- as.numeric(t(mu)%*%sigma_inv%*%mu)
  lambda_tilde <- as.numeric(2*(mu_bar -D/C)/(E-D^2/C))
  weff <- w_mvp + lambda_tilde / 2 * (sigma_inv%*%mu - D/C*sigma_inv%*%iota)
  return(t(weff))
}
compute_efficient_portfolio(sigma = sigma, mu = colMeans(price_matrix))
compute_efficient_portfolio(sigma, colMeans(price_matrix), mu_bar = 0.25 / 250)
```

## Parameter uncertainty

- Consider a quadratic utility function with risk aversion $\gamma$ and certainty equivalent $$CE(\omega) = \omega'\mu - \frac{\gamma}{2} \omega'\Sigma \omega $$ 
- Maximum expected utility portfolio (under the constraint $\iota'\omega = 1$) is equivalent to the framework above (minimize volatility for a given level of return $\bar\mu$) (the proof is an exercise: show that there is a bijective mapping from $\gamma$ to $\bar\mu$)
- Optimal investment: efficient portfolio $\omega_\gamma(\mu, \Sigma)$
- Econometrician can only invest in $\omega_\gamma(\hat\mu, \hat\Sigma)$
- As a result: Inefficient wealth allocation

## How much does parameter uncertainty matter?
- Certainty equivalent loss is defined as 
$$CEL = CE(\omega_\gamma(\mu, \Sigma)) - E\left(CE(\omega_\gamma(\hat\mu, \hat\Sigma))\right) \geq 0$$
- Cho (2007) shows that the loss can be approximated by 
$$CEL \approx \frac{\gamma}{2}\times tr\left(\text{cov}(\omega_\gamma(\hat\mu,\hat\Sigma))\hat\Sigma\right)$$
- Depends on the risk-aversion $\gamma$, the variance-covariance matrix of $\omega_\text{eff}(\hat\mu, \hat\Sigma)$ and the return covariance
- Kan and Zhou (2007) and DeMiguel (2009) consider different portfolio weights that reduce the certainty equivalent loss
- It can be shown that the naive portfolio $\frac{1}{N}\iota$ can be optimal if estimation (and model) uncertainty is huge

## How imposing restrictions helps (in theory)

- Consider the Kuhn-Tucker conditions for the minimum variance portfolio with short-selling constraints
$$\Sigma\omega - \lambda = \lambda^0\iota \text{ where } \lambda_i \geq0 \text{ and } \lambda_i = 0 \text{ if } \omega_i >0$$
- Let $\omega_\text{no short}$ denote the solution to the problem above
- Set $\tilde \Sigma = \Sigma - \left(\lambda\iota' + \iota \lambda'\right)$. Then, $\omega_\text{no short}$ is the (unconstrained) minimum variance portfolio for $\tilde\Sigma$ because:
$$\begin{aligned}
\tilde\Sigma \omega_\text{no short} &= \Sigma \omega_\text{no short}  -\lambda\underbrace{\iota'\omega_\text{no short}}_{= 1} + \iota \underbrace{\lambda'\omega_\text{no short}}_{= 0}\\
&= \Sigma \omega_\text{no short} - \lambda = \lambda^0\iota
\end{aligned}$$

- What is so special about $\tilde\Sigma$ instead of $\Sigma$? Consider the (unconstrained) first-order condition again. If stock $i$'s marginal contribution to the portfolio variance is large, the weights $\omega_i$ are reduced (become negative)
$$\Sigma\omega = \lambda_0\iota \Leftrightarrow \sum\limits_{j=1}^N\omega_j\Sigma_{i,j} = \lambda_0 \forall i$$
- With $\tilde\Sigma$, assets which would imply negative weights are treated as if the variance is reduced by $2\lambda_i$ and covariance with asset $j$ is reduced by $\lambda_i + \lambda_j$

## How imposing restrictions helps (in R) 

- Two commonly used approaches are naive diversification $\omega_\text{naive} = \frac{1}{N}\iota$ and short-sell constrained portfolios
$$\omega_\text{no short} = \arg\max \omega'\mu - \frac{\gamma}{2}\omega'\Sigma\omega \quad \text{s.t. }\omega'\iota = 1 \text{ and } \omega_i \geq0 \quad\forall i$$
- No closed-form solution exists to compute $\omega_\text{no short}$
- Function `solve.QP` from package `quadprog` delivers the numerical solution to quadratic programming problems of the form 
$$\min(-\mu' \omega + 1/2 \omega' \Sigma \omega) \text{ s.t. } A' \omega >= b_0$$
- The function takes argument `meq` for the number of equality constraints
- $A$ for $\omega_\text{no short}$ is $(N +1 \times N)$ and of the form (`meq` = 1)
$$\begin{aligned}A = \begin{pmatrix}1 & 1& \ldots&1 \\1 & 0 &\ldots&0\\0 & 1 &\ldots&0\\\vdots&&\ddots&\vdots\\0&0&\ldots&1\end{pmatrix}'\qquad b_0 = \begin{pmatrix}1\\0\\0\\\vdots\\0\end{pmatrix}\end{aligned}$$
- For more complex optimization routines in R, [this link (optimization task view)](https://cran.r-project.org/web/views/Optimization.html) helps

## How imposing restrictions helps (in R)

- The code snippet below shows how to implement quadratic programming

```{r}
# install.packages("quadprog")
N <- ncol(sigma)                  
A <- t(rbind(1,                  # vector of ones 
             diag(N)))           # identity matrix
cbind(t(A), c(1, rep(0, N)))
solution <- quadprog::solve.QP(Dmat = 2 * sigma, # 2 resembles gamma
                               dvec = colMeans(price_matrix), 
                               Amat = A, 
                               bvec = c(1, rep(0, N)), 
                               meq = 1)
names(solution) # Output is a list of relevant values. You can access list elements with $
solution$solution
```


# Empirical evidence for the CAPM

## Main implication: Capital asset pricing model
- **If** all investors share beliefs about $\Sigma$ and $\mu$ **and** can borrow and invest without limits  at $r_f$, **everybody** invests a fraction of her wealth in $\omega_\text{tgc}$ and in the risk-free rate (two mutual fund theorem) 
- The tangency portfolio is the market portfolio $\omega_\text{m}$ and the individual weights of asset $i$ are just 
$$\omega_\text{tgc, i} = \omega_\text{m, i} = \frac{P_iSCO_i}{\sum\limits_{j=1}^N P_jSCO_j}$$ where $SCO_j$ is the number of shares outstanding of a stock $i$.
- To align the two results, the capital asset pricing model imposes constraints for the expected return of stock $i$
$$E(r_i) - r_f = \underbrace{\frac{Cov(r_i, r_m)}{\text{Var}(r_m)}}_{\beta_i} E(r_m - r_f)$$
- Expected returns are entirely determined by the price of risk (market risk premium) and the comovement of asset $i$ with the market, $\beta_i$.

## The US Stock market

- Large parts of the academic literature focus on US stock markets
- Stocks are listed on US exchanges (NYSE, AMEX, NASDAQ, and some smaller ones)
- Extensive data on prices and trading activity is provided by the Center for Research in Security Prices (CRSP), maintained by the University of Chicago, Booth School of Business
- Full sample ranges from December 1925 and is continuously updated
-  A large part of the following computations is based on the textbook [*Empirical Asset Pricing: The Cross Section of Stock Returns*](https://www.wiley.com/en-dk/Empirical+Asset+Pricing:+The+Cross+Section+of+Stock+Returns-p-9781118095041) (available via library from reading list in Absalon)

### How to get the data

- Note that in the textbook, we explain how to get CRSP data from the WRDS interface. This does not work on the KU campus!
- Consult Absalon for a detailed description of the CRSP sample. As KU students you have direct access to the data. \textcolor{red}{Familiarize yourself with CRSP.}

## Composition of the CRSP sample (Chapter 7, Bali, Murray, Engle)

- Familiarize yourself with the cleaning steps in the exercise on the CRSP sample
- Monthly processed (!) data is available in `tidy_finance_r.sqlite` and `tidy_finance_python.sqlite` file in Absalon
- Only contains US stocks (`shrcd%in%c(10,11)`)
- Variable `exchcd` determines listing exchanges, `siccd` lists industry

```{r}
library(RSQLite)
tidy_finance <- dbConnect(SQLite(), "../data/tidy_finance_r.sqlite", extended_types = TRUE) # Connect to sql
crsp_monthly <- tbl(tidy_finance, "crsp_monthly") |> collect()
```

```{python}
#| eval: false
import pandas as pd
import sqlite3

tidy_finance_python = sqlite3.connect(
  database="../data/tidy_finance_python.sqlite"
)

crsp_monthly = (pd.read_sql_query(
  sql=("SELECT permno, month, industry, ret_excess " 
       "FROM crsp_monthly"),
  con=tidy_finance_python,
  parse_dates={"month"})
  .dropna()
)
```

## Industry classification

- We adjust the market capitalization values for inflation using the [Consumer Price Index](www.bls.gov/cpi/data.htm) (CPI, All Urban Consumer series) from the Bureau of Labor Statistics website.

```{r}
#| layout-ncol: 2

crsp_monthly |> count(exchange, date) |>
  ggplot(aes(x = date, y = n, color = exchange)) + geom_line() +
  labs(x = NULL, y = NULL, color = NULL, title = "Monthly number of securities by listing exchange")

crsp_monthly |>
  left_join(tbl(tidy_finance, "cpi_monthly") |> collect(), join_by(month)) |> 
  group_by(month, industry) |> 
  summarize(mktcap = sum(mktcap / cpi) / 1000000) |> ungroup() |> 
  ggplot(aes(x = month, y = mktcap, color = industry)) + geom_line() +
  labs(x = NULL, y = NULL, color = NULL, title = "Market Cap by industry (in trillion USD)")
```

## Compute returns in the CRSP sample

- Monthly stock returns are found in the `RET` field in CRSP
- Problems occur for delistings which require (tedious) adjustments
- Bali, Murray, Engle explain these adjustments (see code snippet below)

```{r}
#| eval: false
raw_crsp_monthly |> # Process Raw file (sqlite contains processed data already)
  mutate(ret_adj = case_when(
    is.na(dlstcd) ~ ret,
    !is.na(dlstcd) & !is.na(dlret) ~ dlret,
    dlstcd %in% c(500, 520, 580, 584) | (dlstcd >= 551 & dlstcd <= 574) ~ -0.30,
    dlstcd == 100 ~ ret,
    TRUE ~ -1)) |>
  select(-c(dlret, dlstcd)) 
```

- *Note*: The `tidy_finance_*.sqlite` data already contains the processed CRSP file with excess and raw returns. For the sake of simplicity, I removed the delisting information (consult the exercises for more details)

## Risk-free rate

- Excess return for a stock is the difference between the stock return and the return on the risk-free security over the same period 
- Monthly risk-free security return data from [Ken
French’s data library](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html)

```{r}
factors_ff_monthly <- tbl(tidy_finance, "factors_ff3_monthly") |> collect()
factors_ff_monthly |> ggplot(aes(x = month, y = 100 * rf)) + 
  geom_line() +  labs(x = NULL, y = NULL, title = "Risk free rate (in percent)") 
```

## Excess returns

- We mainly work with adjusted *excess* returns
- The table below illustrates the time-series averages of cross-sectional means from the entire CRSP sample

```{r}
crsp_monthly |> 
  group_by(month) |>
  summarise(across(ret_excess, 
                   list(mean = mean, sd = sd, min = min, 
                        q25 = ~quantile(., 0.25), 
                        median = median, 
                        q75 = ~quantile(., 0.75), max = max),
                   .names = "{.fn} return")) |>
  summarise(across(-month, mean)) |> 
  kableExtra::kable()
```

## The market factor 

- The market risk premia is the market excess return $z_t = r_{m,t} - r_{f,t}$ 
- We proxy for the market as the value-weighted portfolio of all US-based common stocks in CRSP 
- This aggregation is already provided by Kenneth French on his homepage
- Sharpe ratio is computed as $\frac{\hat{\mu_z}}{\hat{\sigma}_z}$

```{r}
factors_ff_monthly |> 
    mutate(mkt_excess = 100 * mkt_excess) |> 
    summarise(across(mkt_excess, list(mean = ~ 12 * mean(.), 
                                      sd = ~sqrt(12) * sd(.), 
                                      Sharpe = ~sqrt(12) * mean(.)/sd(.)),
                     .names = "{.fn} (annualized)")) |>
  kableExtra::kable()
```

## Estimating the CAPM

- The CAPM of Sharpe (1964), Lintner (1965), and Mossin (1966) originates the literature on asset pricing models
- The CAPM is an equilibrium model in a single-period economy
- Asset risk is the covariance of its return with the market portfolio return
- The higher the co-movement the less desirable the asset is, hence, the asset price is lower and the expected return is higher 
- Risk premium, or the market price of risk, is the expected value of the excess market return
- The market price of risk, common to all assets, is set in equilibrium by the risk aversion of investors

## Regression specification of stock $i$

$$r_{i,t} - r_{f,t} = \alpha_i + \underbrace{\frac{Cov(r_i, r_m)}{\text{Var}(r_m)}}_{\beta_i} (r_{m,t} - r_{f,t}) + \varepsilon_{i, t}$$

- For the econometric analysis of the model, we assume that innovations $\varepsilon_{i, t}$ are independently and identically distributed (IID) through time and jointly multivariate normal
- Expected returns are entirely determined by the price of risk (market risk premium) and the co-movement of asset $i$ with the market, $\beta_i$
- To determine whether the portfolio/asset generates abnormal returns relative to the CAPM risk model, we evaluate whether the fitted intercept coefficient $\alpha_i$, which serves as the estimate of the average abnormal return per period, is statistically distinguishable from zero
- \textcolor{red}{Discuss:} What are the questionable assumptions behind the baseline regression framework? 

## CAPM - Example

- Simple linear regression (OLS) either via `solve(t(X)%*%X)%*%t(X)%*%Y` or `lm()`
```{r}
apple_monthly <- crsp_monthly |> filter(permno == 19764) |> # 19764 = APPLE
  left_join(factors_ff_monthly) 
capm_regression <- lm(ret_excess ~ mkt_excess, data = apple_monthly) # linear model
capm_regression |> broom::tidy() |> knitr::kable()
apple_monthly |> ggplot(aes(x = mkt_excess, y = ret_excess)) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  labs(x = "Market Excess Returns", y = "AAPL Excess Returns")
```

## Computing beta for the CRSP universe

- Market beta for month $t$ is estimated with data from prior to and including $t$, e.g. 5 years of monthly data 
- Rolling-window regressions are straightforward from a methodological perspective but tricky to implement  
- Exercises: conduct rolling window regression of beta for the entire CRSP universe

:::: {.columns}
::: {.column width="45%"}
```{r}
#| eval: false
roll_capm_estimation <- function(data, months, min_obs) {
  data <- data |>
    arrange(month)

  betas <- slide_period_vec(
    .x = data,
    .i = data$month,
    .period = "month",
    .f = ~ estimate_capm(., min_obs),
    .before = months - 1,
    .complete = FALSE
  )

  return(tibble(
    month = unique(data$month),
    beta = betas
  ))
}
```
:::
::: {.column width="45%"}
```{python}
#| eval: false
def roll_capm_estimation(data, window_size, min_obs):
  
    data = data.sort_values("month")

    result = (RollingOLS.from_formula(
      formula="ret_excess ~ mkt_excess",
      data=data,
      window=window_size,
      min_nobs=min_obs
      )
      .fit()
      .params["mkt_excess"]
    )
    
    result.index = data.index
    
    return result
```
::: 
::::

## CAPM beta in the industry-cross section
```{r}
beta <- tbl(tidy_finance, "beta") |> collect() |> 
  inner_join(crsp_monthly, join_by(month, permno)) |> drop_na(beta_monthly)
beta  |> group_by(industry, permno) |> summarise(beta = mean(beta_monthly)) |> 
   ggplot(aes(x = reorder(industry, beta, FUN = median), y = beta)) +
  geom_boxplot() + coord_flip() +
  labs(x = NULL, y = NULL, title = "Firm-specific beta distributions by industry"
  )
```

## Portfolio sorts with rolling windows for CRSP

- \textcolor{red}{Discuss: What does the CAPM imply?}
- Univariate portfolio analysis (nonparametric technique):

1. Calculate breakpoints (at $t-1$) to divide the sample into portfolios
1. Calculate the average return $r_{p,t}$ within each portfolio for each period $t$
1. Examine variation in these average values of $r_{p,t}$  across the different
portfolios
  
```{r}
beta_portfolios <- beta |> # A stylized example (note time convention in exercise!)
  group_by(month) |>
  mutate(breakpoint = median(beta_monthly),
         portfolio = case_when(beta_monthly <= breakpoint ~ "low",
                               beta_monthly > breakpoint ~ "high")) |>
  group_by(month, portfolio) |>
  summarize(ret = weighted.mean(ret_excess, mktcap_lag), .groups = "drop")

beta_portfolios |> # Positive alpha?
  pivot_wider(names_from = portfolio, values_from = ret) |> 
  mutate(long_short = high - low) |> 
  left_join(factors_ff_monthly) |> 
  lm(long_short ~ mkt_excess, data = _) |> broom::tidy()
```

## Bad news for CAPM?

![](figures/alpha_beta_sorts.png){fig-align="center" width="50%"}

- Figure shows decile portfolio sorts based on *lag* beta
- Portfolio 10 corresponds to the highest beta decile
- Each bar corresponds to the CAPM alpha of the value-weighted portfolio performance

## How to use (CAPM) factor structure for portfolio optimization

- Suppose the CAPM holds: implied factor structure for expected excess returns is
$$\begin{aligned}\mu &= E(r) = r_f + \beta E(r_m - r_f)\\\Sigma &=\sigma^2_m\beta\beta' + \Sigma_\varepsilon
\end{aligned}$$
- If $\Sigma_\varepsilon$ is a diagonal matrix, estimation of $\Sigma$ requires only $2N + 1$ instead of $N(N-1)/2$ parameters
- Practical recipe for portfolio optimization with general factor structure:
1. Estimate $\beta_i$ for each asset
1. Estimate market risk premium $E(r_m - r_f)$
1. (Univariate) estimation of the elements of $\Sigma_{\varepsilon}$ based on residualized returns
$$\hat\varepsilon_{i,t} = r_{t,i} - r_f - \hat\beta_i(r_{m,t} - r_f)$$
4. Replace sample estimates $\hat\mu$ and $\hat\Sigma$ with the theoretically implied values computed above to choose
$$\omega = \arg\max \hat\mu'\omega - \frac{\gamma}{2}\omega'\hat\Sigma\omega$$

## Testing the CAPM

- Suppose we want to test if the CAPM holds jointly for $N$ assets
- The CAPM implies that all elements of the $(N \times 1)$ vector $\alpha$ are zero in the joint regression framework
$$Z_t = \alpha + \beta Z_{m,t} + \varepsilon_t$$
where $\beta$ is the $(N \times 1)$ vector of market betas (if $\alpha$ is zero, then the market portfolio is the tangency portfolio) and $Z_{i,t}$ denotes excess returns
- Standard ordinary least squares (OLS) estimation delivers
$$\hat\alpha = \hat\mu - \hat\beta\hat\mu_m$$ with
$$\hat\beta = \frac{\sum (Z_t - \hat\mu)(Z_{m,t} - \hat\mu_m)}{\sum (Z_{m,t} - \hat\mu_m)^2}\text{ and } \hat\mu_i = \frac{1}{T}\sum\limits_{t=1}^TZ_{i,t}$$

- The Wald-test statistic of the null hypothesis $H_0: \alpha = 0$ is
$J = \hat\alpha'\left(Var(\hat\alpha)\right)^{-1}\hat\alpha$

## Testing the CAPM

- MacKinlay (1987) and Gibbons, Ross, and Shanken (1989) developed the finite-sample distribution of $J$ which yields
$$J = \frac{T-N-1}{N}\left(1 + \frac{\hat \mu_m ^2}{\hat\sigma_m^2}\right)^{-1}\hat\alpha'\hat\Sigma^{-1}\hat\alpha$$
where $\hat\Sigma = Cov(\hat\varepsilon)$ and $\hat\sigma$ is the standard deviation of the market excess returns.
- Under the null hypothesis, $J$ is unconditionally distributed central $F$ with
$N$ degrees of freedom in the numerator and $(T-N-1)$ degrees of freedom
in the denominator

### Exercises - compute the test statistics for a sample of portfolio returns

1. Compute the MLE estimates and the test statistic `J <-  `
1. Evaluate the $p$-value `pf(J, T, T - N - 1)` with appropriate degrees of freedom

- For more information, see Chapter 4 of *The Econometrics of Financial Markets*

## Fama Mac-Beth regressions
- Instead of focusing on the mean-variance efficiency of the market portfolio, the CAPM also implies a linear relationship between expected returns and market betas which completely explains the cross-section of expected returns
- Portfolio sorts already revealed that this may not be the case
- These implications can also be tested using a cross-sectional regression methodology (Fama and MacBeth, 1973)
- Basic idea:
For each cross-section of returns (e.g., each month), project asset returns on factor exposures or characteristics that resemble exposure to a risk factor and then aggregate the estimates in the time dimension
- E.g., first, for each month $t$, estimate
$$Z_t = \gamma_{0,t}\iota + \gamma_{1,t}\hat\beta + \eta_t$$
- Then, we analyse time series of $\hat\gamma_{0,t}$ and $\hat\gamma_{1,t}$.
- CAPM implies that $E(\gamma_{0,t}) = 0$ (no mispricing) and $E(\gamma_{1,t})>0$ (positive market premium)
- In most applications we use $\hat\beta \Rightarrow$ errors-in-variables problem (Shanken, 1992)

## Unobservability of the market portfolio

- Gibbons, Ross, and Shankens test focuses on the mean-variance efficiency of the market portfolio
- Most tests use a value- or equal-weighted basket of NYSE and AMEX stocks as the
market proxy, whereas theoretically, the market portfolio contains all assets
- Roll (1977) emphasizes that tests of the CAPM only reject the mean-variance efficiency of the proxy and that the model might not be rejected if the return on the true market portfolio were used

Implications of the overwhelming evidence against the CAPM?

- Replace CAPM with multifactor models with several sources of risk
- Maybe the evidence against the CAPM is overstated because of mismeasurement of the market portfolio, improper neglect of
conditioning information, data-snooping, or sample-selection bias
- What if no risk-based model can explain the anomalies of the stock market
behavior (behavioral finance)?

## Theoretical drawbacks of the CAPM

- The CAPM assumes that the average investor cares only about the performance of the investment portfolio but wealth could emerge from other sources and higher-order risks could play a role
- The CAPM assumes a static one-period model. In Merton's (1973) ICAPM, the demand for risky assets is attributed not only to the mean-variance component, as in the CAPM, but also to hedging against unfavorable shifts in the investment opportunity set
- Empirically, the poor performance of the single factor CAPM motivated a search for multifactor models
