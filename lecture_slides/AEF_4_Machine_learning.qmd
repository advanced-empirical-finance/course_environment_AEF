---
title: "Advanced Empirical Finance: Topics and Data Science"
author: "Stefan Voigt"
institute: "University of Copenhagen and Danish Finance Institute"
date: today
date-format: "[Spring] YYYY"
format: beamer
pdf-engine: lualatex

theme: metropolis
fontsize: 10pt
mainfont: Fira Sans
monofont: Source Code Pro
monofontoptions: 
  - Scale=0.9
mathfont: firamath

knitr:
  opts_chunk: 
    size: "tiny"
    fig.width: 8
    fig.height: 4
    fig.align: "center"
    out.width: "60%"
    R.options:
      tibble.print_max : 4
      tibble.print_min : 4
      formatR.indent: 2
      digits : 3
      crayon.enabled: FALSE

execute:
  echo: TRUE
  cache: FALSE
  eval: TRUE
  message: FALSE
  warning: FALSE

header-includes: |-
  \metroset{progressbar=frametitle} 
  \usepackage{graphicx} 
  \usepackage{booktabs} 
  \usepackage{amsmath,amsfonts,amssymb} 
  \usepackage{listings} 
  \setbeamercolor{progress bar}{fg=red} 
  \setbeamercolor{frametitle}{fg=black,bg=white} 
  \setbeamercolor{background canvas}{bg=white} 
  \usepackage{hyperref} 
  \hypersetup{colorlinks=false}
  \setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
---

```{r}
#| include: false
source("pre_session_script.R")
```

```{python}
#| include: false
from plotnine import theme_set, theme, theme_bw
theme_set(theme_bw() + theme(legend_position="bottom"))
```

# Machine learning

## What is Machine learning?

:::: {.columns}
::: {.column width="80%"}

*The definition of "machine learning" is inchoate and is often context specific. We use the term to describe **(i)** a diverse collection of high-dimensional models for statistical prediction, combined with **(ii)** so-called "regularization" methods for model selection and mitigation of overfit and **(iii)** efficient algorithms for searching among a vast number of potential model specifications. (Gu et al. 2020)*

- (i) select between small simplistic and complex ML models 
- (i) Focus on predictive accuracy
- (ii) selecting from multiple models in-sample leads to overfitting and poor out-of-sample performance
- (ii) "regularization" methods for model selection
- (iii) challenge in terms of computational effort

:::
::: {.column width="20%"}

![]("figures/black_automated_stock_market.png"){width=100% fig-align="center"}

:::
::::

## What makes ML in Finance special? 

:::: {.columns}
::: {.column width="80%"}

### Challenges (Israel, Kelly, Moskowitz, 2019)

- Limited data (left-hand side limited by $T$)
- Markets evolve and thus even lower effective sample size
- By market efficiency: small signal-to-noise ratio (limited predictability)
- Data potentially unstructured (company announcements)


### But...

- Machine learning methods on their own do not identify fundamental associations among asset prices and conditioning variables

:::
::: {.column width="20%"}

![]("figures/satellite_image_parking_lot.jpg")

![]("figures/satellite_image_oil_tanks.jpg")

:::
::::


## Overview: Empirical Asset Pricing via Machine Learning

- Familiarize yourself with the paper "Empirical Asset Pricing via Machine
Learning" by Gu et al. (2020)

- comparative analysis of machine learning methods for the canonical problem
of measuring asset risk premiums
- "We demonstrate large economic gains to investors using *machine learning forecasts*, in some cases doubling the performance of leading regression-based strategies from the literature."

## Machine learning roadmap

1. Bias-Variance Trade-off
1. Penalized Linear Regressions (Ridge and Lasso)
1. Regression Trees and Random Forests
1. Neural Networks
1. Advanced case studies and applications

### Your task: 

- Return prediction for all CRSP-listed stocks 
- Large set of macroeconomic predictors 
- Hundreds of predictive firm and economic characteristics
- You should study Gu et al. (2020) in depth!
- **Exercises:** Prepare the dataset as explained in Section 2.1 of Gu et al. (2020)

# Bias-Variance Trade-off

## Unbiased, linear estimators 

$$E_t\left(r_{i, t+1}\right) = g^\star(x_{i, t}) \overset{??}{=} \beta'x_{i,t} $$

- Machine learning prescribes a vast collection of high-dimensional models that attempt to predict future quantities of interest while imposing regularization
- We know: OLS is the best linear unbiased estimator (BLUE)
- "Best" = the lowest variance estimator among all other *unbiased linear* estimators
- Requiring the estimator to be *linear* is binding since *nonlinear* estimators exist (e.g., neural networks or regression trees)
- Likewise, *unbiased* is crucial since *biased* estimators do exist

### Biased estimators? 

- *Shrinkage* methods: the variance of the OLS estimator can be high as OLS coefficients are unregulated
- If judged by Mean Squared Error (MSPE), biased estimators could be more attractive if they produce substantially smaller variance than OLS

## Shortcomings of OLS

- Let $\beta$ denote the true regression coefficient and let $\hat\beta = \left(X'X\right)^{-1}X'y$, where $X$ is a $(T \times N)$ matrix of explanatory variables
- Then, the variance of the (unbiased) OLS estimate $\hat\beta$ is given by 
$$\begin{aligned}
Var\left(\hat\beta\right) &= E\left(\left(\hat\beta-\beta\right)\left(\hat\beta - \beta\right)'\right)\\
&=E\left((X'X)^{-1}X'\varepsilon\varepsilon'X(X'X)^{-1}\right)\\
&=\sigma_\varepsilon^2 E\left((X'X)^{-1}\right)
\end{aligned}$$
where $\varepsilon$ is the vector of residuals and $\sigma_\varepsilon^2$ is the variance of the error term
- When the predictors are highly correlated, the term $(X'X)^{-1}$ quickly explodes
- Even worse: the OLS solution is not unique if $X$ is not of full rank


### OLS in a prediction context

1. restrictive
1. may provide poor predictions, may be subject to *over-fitting*
1. does not penalize for model complexity and could be difficult to interpret


## The Bias-Variance Trade-off

- Assume the model
$$y = f(x) + \varepsilon, \quad \varepsilon \sim (0, \sigma_\varepsilon^2)$$
- $\hat\beta^\text{ols}$ has a host of well-known properties (Gauss-Markov)
- But: Can we choose $\hat f(x)$ to fit future observations well?
- MSPE depends on the model as follows: $$\begin{aligned}
E(\hat{\varepsilon}^2)&=E((y-\hat{f}(\textbf{x}))^2)=E((f(\textbf{x})+\varepsilon-\hat{f}(\textbf{x}))^2)\\
&= \underbrace{E((f(\textbf{x})-\hat{f}(\textbf{x}))^2)}_{\text{total quadratic error}}+\underbrace{E(\varepsilon^2)}_{\text{irreducible error}} \\
&= E\left(\hat{f}(\textbf{x})^2\right)+E\left(f(\textbf{x})^2\right)-2E\left(f(\textbf{x})\hat{f}(\textbf{x})\right)+\sigma_\varepsilon^2\\
&=E\left(\hat{f}(\textbf{x})^2\right)+f(\textbf{x})^2-2f(\textbf{x})E\left(\hat{f}(\textbf{x})\right)+\sigma_\varepsilon^2\\
&=\underbrace{\text{Var}\left(\hat{f}(\textbf{x})\right)}_{\text{variance of model}}+ \underbrace{E\left((f(\textbf{x})-\hat{f}(\textbf{x}))\right)^2}_{\text{squared bias}} +\sigma_\varepsilon^2 
\end{aligned}$$
- A biased estimator with small variance may have a lower MSPE than an unbiased estimator

## Over-fitting example 

- 100 monthly manufacturing industry excess returns
- Estimate a polynomial regression $$r_{t} = \alpha + \sum\limits_{p=1}^P \beta_pt^p $$ where $t$ is a time index, ranging from $1$ to $60$
- Evaluate the performance in-sample and out-of-sample for $P = 1, 2, 3, 5, 20$

```{r}
#| echo: false

library(RSQLite)
tidy_finance <- dbConnect(SQLite(), "../data/tidy_finance_r.sqlite", extended_types = TRUE)

data <- tbl(tidy_finance, "industries_ff_monthly") |>
    select(month, returns = manuf) |>
    collect() |>
    mutate(time = as.integer(month)) |> 
    tail(100)

# Split the data into training and testing sets
in_sample_size <- 60
train_set <- data |> slice_head(n = in_sample_size)
test_set <- data |> slice_tail(n = nrow(data) - in_sample_size)

# Function to fit polynomial regression models
fit_polynomial <- function(degree, data = train_set) {
  formula <- as.formula(paste("returns ~ poly(time, ", degree, ", raw = TRUE)", sep = ""))
  model <- lm(formula, data)
  return(model)
}

results <- tibble(degree = c(1, 2, 3, 5, 20)) |> 
  mutate(lm = map(degree, fit_polynomial),
         lm_train = map(lm, predict, newdata = train_set, rankdeficient = "NA"),
         lm_test = map(lm, predict, newdata = test_set, rankdeficient = "NA"),
         lm_predictions = map2(lm_train, lm_test, 
                               ~tibble(time = data$time,
                                       returns =c(.x,.y)))) |>
  unnest(lm_predictions) |> 
  mutate(time = as.Date(time))

# Plot the results
ggplot(data |> mutate(time = as.Date(time)), 
       aes(x = time, y = returns)) +
  geom_point() +
  scale_y_continuous(labels = scales::percent,
                     limits = c(min(data$returns), max(data$returns))) +
  geom_line(data = results, aes(color = as_factor(degree))) +
  annotate("rect",
           xmin = test_set |> pull(time) |> min() |> as.Date(),
           xmax = test_set |> pull(time) |> max() |> as.Date(),
           ymin = -Inf, ymax = Inf,
           alpha = 0.5, fill = "grey70"
  ) +
  labs(x = NULL,
       y = "Return",
       color = NULL)
```


## Ridge Regression
- Introduced by Hoerl and Kennard (1970a, 1970b)
- Impose a penalty on the $L_2$ norm of the parameters $\hat\beta$ such that for $c\geq 0$ the estimation takes the form
$$\hat\beta^\text{ridge} = \arg\min_\beta \left(y - X\beta\right)'\left(y - X\beta\right)\text{ s.t. } \beta'\beta \leq c$$
- Standard optimization procedure yields
$$\hat\beta^\text{ridge} = \left(X'X + \lambda I\right)^{-1}X'y$$
- Hyper parameter $\lambda$ $(c)$ controls the amount of regularization
- Note that $\hat\beta^\text{ridge} = \hat\beta^\text{ols}$ for $\lambda = 0$ $(c\rightarrow \infty)$ and $\hat\beta^\text{ridge} \rightarrow 0$ for $\lambda\rightarrow \infty$ $(c\rightarrow 0)$
- $\left(X'X + \lambda I\right)$ is non-singular even if $X'X$ is
- *Note: * Usually, the intercept is not penalized (in practice: demean $y$)

## Ridge Regression
- Let $D := X'X$
$$
\begin{aligned}
\hat\beta^\text{ridge} &= \left(X'X + \lambda I\right)^{-1}X'y\\
&= \left(D + \lambda I\right)^{-1}DD^{-1}X'y\\
&= \left(D\left(I + \lambda D^{-1}\right)\right)^{-1}D\hat\beta^\text{ols}\\
&= \left(I + \lambda D^{-1}\right)^{-1}D^{-1}D\hat\beta^\text{ols} = \left(I + \lambda D\right)^{-1}\hat\beta^\text{ols}
\end{aligned}
$$
- $\hat\beta^\text{ridge}$ is biased because $E(\hat\beta^\text{ridge} - \beta) \neq 0$ for $\lambda\neq0$
- *But* at the same time (under homoscedastic error terms)
$$\text{Var}(\hat\beta^\text{ridge}) = \sigma_\varepsilon^2 \left(D + \lambda I\right)^{-1}X'X\left(D + \lambda I\right)^{-1}$$
- You can show that $\text{Var}(\hat\beta^\text{ridge}) \leq \text{Var}(\hat\beta^\text{ols})$
- Trade-off between bias and variance of the estimator!


## Bias variance trade-off with the ridge trace
  
```{r}
#| echo: false
D <- 1
sigma_e <- 1
values <- tibble(lambda = seq(from = 0, to = 5, by = 0.1)) |> 
  mutate(bias = (lambda / (1+lambda))^2,
         variance = sigma_e^2 * (D + lambda)^-2 * D,
         mspe = bias + variance) |> 
  pivot_longer(-lambda, 
               names_to = "variable", 
               values_to = "value") 

ggplot(values, aes(x = lambda, 
                   y = value, 
                   color = variable)) +
  geom_point() +
  geom_vline(xintercept = values |> 
               filter(variable == "mspe") |> 
               filter(value == min(value)) |>
               pull(lambda), 
             linetype = "dotted") +
  labs(x = paste("Penalty (Lambda)"),
       y = NULL,
       color = NULL,
       title = "Bias-Variance Trade-off") +
  guides(x = "none", y = "none") +
  scale_color_discrete(labels = c("Bias (Squared)", "MSPE", "Variance"))

```

## A case study

- Data for case study: macroeconomic predictors from for the paper "A Comprehensive Look at The Empirical Performance of Equity Premium Prediction" (Goyal, 2008)
- Monthly variables that have been suggested as good predictors for the equity premium: Dividend Price Ratio, Earnings Price Ratio, Stock Variance, Net Equity Expansion, Treasury Bill rate, and inflation
- Monthly Fama-French 3-factor returns (market, small-minus-big, and high-minus-low book-to-market valuation sorts)
- Monthly q-factor returns from Hou, Xue, and Zhang (2015)
- Monthly portfolio returns from 10 different industries according to the definition from Kenneth French's homepage
- The regression specification is
$$\underbrace{r_{i, t}}_{\text{industry } i} = \gamma_{i, 0} + \gamma_{i, 1} \underbrace{x_{t-1}}_{\text{macro} \times \text{factor}} + \varepsilon_t$$
- Package `glmnet` fits generalized linear models via penalized maximum likelihood
- **Exercise:** Implement Ridge on your own before using `glmnet`

```{r}
#| echo: false
library(RSQLite)
library(tidyverse)

tidy_finance <- dbConnect(
  SQLite(),
  "../data/tidy_finance_r.sqlite",
  extended_types = TRUE
)

factors_ff3_monthly <- tbl(tidy_finance, "factors_ff3_monthly") |>
  collect() |>
  rename_with(~ str_c("factor_ff_", .), -month)

factors_q_monthly <- tbl(tidy_finance, "factors_q_monthly") |>
  collect() |>
  rename_with(~ str_c("factor_q_", .), -month)

macro_predictors <- tbl(tidy_finance, "macro_predictors") |>
  collect() |>
  rename_with(~ str_c("macro_", .), -month) |>
  select(-macro_rp_div)

industries_ff_monthly <- tbl(tidy_finance, "industries_ff_monthly") |>
  collect() |>
  pivot_longer(-month,
    names_to = "industry", values_to = "ret"
  ) |>
  arrange(desc(industry)) |> 
  mutate(industry = as_factor(industry))

data <- industries_ff_monthly |>
  left_join(factors_ff3_monthly, join_by(month)) |>
  left_join(factors_q_monthly, join_by(month)) |>
  left_join(macro_predictors, join_by(month)) |>
  mutate(
    ret = ret - factor_ff_rf
  ) |>
  select(month, industry, ret_excess = ret, everything()) |>
  drop_na()
```


## Ridge trace

- Below I visualize the *ridge trace* for different values of the penalty $\lambda$
- To keep things simple, I restrict the sample to the manufacturing portfolio

```{r}
#| echo: false
y <- data |> filter(industry == "manuf") |> pull(ret_excess)
x <- data |> filter(industry == "manuf") |> select(factor_ff_mkt_excess:last_col()) |> as.matrix()

library(glmnet)
fit_ridge <- glmnet(x, y, alpha = 0) # Model
```

```{r, echo = FALSE}
broom::tidy(fit_ridge) |>
  filter(term != "(Intercept)") |>
  ggplot(aes(x=lambda, y = estimate, color = term)) +
  geom_line() +
  geom_hline(data = broom::tidy(fit_ridge) |>
  filter(term != "(Intercept)")  |> filter(lambda == min(lambda)), aes(yintercept = estimate, color = term), linetype = "dotted") + 
  scale_x_log10() +
  labs(x = "Lambda", y = "Estimate", color = "Variable") +
  theme(legend.position = "None")
```

## The Lasso (Tibshirani, 1996)

- Obvious "drawback" of Ridge regression: no variable selection
- Instead of proportional shrinkage (Ridge), the Lasso (least absolute shrinkage and selection operator) translates each
coefficient by a constant factor $\lambda$, truncating at zero
- Lasso implements a $L_1$ penalization on the parameters s.t. $\|\beta\|_1 := \sum_k|\beta_k| \leq c$

$$\hat\beta^\text{Lasso} = \arg\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right)\text{ s.t. } \sum\limits_{k=1}^K|\beta_k| < c(\lambda).$$

- Equivalent optimization problem for a hyperparameter $\lambda$:
$$\hat\beta_\lambda^\text{Lasso} = \arg\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right) + \lambda\sum\limits_{k=1}^K|\beta_k|.$$
- No closed-form solution but efficient algorithms (`glmnet`)
- Also here: typically no penalization on the intercept term
- **Exercise: ** Implement Lasso estimation on your own before using `glmnet`

## Lasso with glmnet
```{r}
#| echo: false
fit_lasso <- glmnet(x, y, alpha = 1)
```

```{r, echo = FALSE}
broom::tidy(fit_lasso) |>
  filter(term != "(Intercept)") |>
  ggplot(aes(x=lambda, y = estimate, color = term)) +
  geom_line() +
  geom_hline(data = broom::tidy(fit_lasso) |>
  filter(term != "(Intercept)") |> filter(lambda == min(lambda)), aes(yintercept = estimate, color = term), linetype = "dotted") + 
  theme_minimal() +
  scale_x_log10() +
  labs(x = "Lambda", y = "Estimate") +
  theme(legend.position = "None")
```

## Difference between Ridge and Lasso 

![]("figures/lasso.jpg"){width=50% fig-align="center"}

- Ridge can be interpreted as a Bayesian posterior mean with a **Normal** prior on $\beta$
- Lasso can be interpreted as a Bayesian posterior mean with a **Laplace** prior on $\beta$
$$\beta^\text{Ridge} \propto \exp\left(-\frac{\lambda\beta^2}{\sigma}\right) \qquad \beta^\text{Lasso} \propto \frac{\lambda}{2\sigma}\exp\left(-\frac{\lambda|\beta|}{\sigma}\right)$$
- Next step: Elastic net (Zhou & Hastie, 2005) combines $L_1$ and $L_2$ penalization
- Encourages a grouping effect, where strongly correlated predictors tend to
be in or out of the model together.
$$\hat\beta^\text{EN} = \arg\min_\beta \left(Y - X\beta\right)'\left(Y - X\beta\right) + \lambda(1-\rho)\sum\limits_{k=1}^K|\beta_k| +\frac{1}{2}\lambda\rho\sum\limits_{k=1}^K\beta_k^2$$

# Cross-validation 

## "Hyperparameter-tuning" with cross-validation

- Goal: find an algorithm that produces predictors $\hat{y}$ for an outcome $y$ that minimizes the mean squared prediction error:
$$
\mbox{MSPE} = \mbox{E}\left( \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2 \right)
$$
- We can only estimate the MSPE: 
$$
\hat{\mbox{MSPE}} = \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i - y_i\right)^2
$$

1. Because our data is random, the apparent error is a random variable
2. If we train an algorithm on the same dataset that we use to compute the apparent error, we might be overfitting

## Cross-validation
- Cross-validation is a technique that permits us to alleviate both these problems
- Think of the true MSPE as the average of many apparent errors obtained by applying the algorithm to $B$ new random samples of the data, none of them used to train the algorithm
$$
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2 
$$
- Idea: randomly generate smaller datasets that are not used for training and instead used to estimate the true error
- Recall: The goal is to choose hyperparameters $\lambda$ to obtain the smallest MSPE

## Sample split

- Carve out a piece of our dataset and pretend it is an independent dataset: divide it into a _training set_ (blue) and a _test set_ (red)
- Train the algorithm exclusively on the training set and use the test set only for evaluation purposes (not for filtering out rows, not for selecting features, nothing!)
- Typical choices are to use 10%-20% of the data for testing 

![]("figures/cross_validation.png"){width=50% fig-align="center"}

## Validation sample

- To choose from the set of hyperparameters, we further divide our training sample without using our test sample!
- For each set of algorithm parameters being considered, we want an estimate of the MSPE, and then we will choose the parameters with the smallest MSPE

1. Prespecify a grid of hyperparameters
2. Obtain predictors $\hat{y}_i(\lambda)$ to denote the predictors for the used parameters $\lambda$
3. Compute 
$$
\mbox{MSPE}(\lambda) = \frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2 
$$
- With K-fold cross-validation, we do it $K$ times: pick a validation set with $M=N/K$ observations at random and think of these as a random sample $y_1^b, \dots, y_M^b$, with $b=1$

## K-fold cross-validation

- Fit the model in the training set, then compute the apparent error on the independent set
$$
\hat{\mbox{MSPE}}_b(\lambda) = \frac{1}{M}\sum_{i=1}^M \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2 
$$
- Take $K$ samples to reduce the variance of the estimate
- In K-cross validation, we randomly split the observations into $K$ non-overlapping sets:

![]("figures/cross_validation_k.png"){width=50% fig-align="center"}

## K-fold cross-validation

- repeat the calculation above for each of these sets $b=1,\dots,K$ and obtain $\hat{\mbox{MSPE}}_1(\lambda),\dots, \hat{\mbox{MSPE}}_K(\lambda)$ and compute the average
$$
\hat{\mbox{MSPE}}(\lambda) = \frac{1}{K} \sum_{b=1}^K \hat{\mbox{MSPE}}_b(\lambda)
$$
- final step: select the $\lambda$ that minimizes the MS

### Considerations for selecting $K$
- Large values of $K$ are preferable. The training data better imitates the original dataset
- Larger values of $K$ will have much slower computation time
- One way to improve the variance of our final estimate is to take more samples. To do this, pick $K$ sets of some size at random (not necessarily non-overlapping)
- The bootstrap: at each fold, pick observations at random with replacement (which means the same observation can appear twice)


## Machine learning in one expression

- Method defines function class $\mathcal{F}$ (in this case, linear model) and a regularizer $R( f )$ (shrinkage intensity $\lambda$, later: depth of the tree) that expresses the complexity of a function
- Picking the prediction function then involves two steps

1. conditional on a level of complexity, pick the best in-sample loss-minimizing function
$$\min \sum\limits_{i=1}^nL\left(f(x_i),y_i\right)\text{ over } f\in\mathcal{F}\text{ subject to } R(f) \leq c$$
2. estimate the optimal level of complexity $c$ using empirical tuning (cross-validation)

## ML workflows in R and Python

- R and Python provide unparalleled workflows for ML: `tidymodels` and `scikit-learn`

```{mermaid}
%%| echo: False
%%| fig-width: 3

flowchart LR
  A(Recipe) --> C(Model)
  C --> D(Tuning)
  F(Data splits) --> D
  D --> E(Selection)
```

- Preprocessing steps (`recipe` and `transformers`)
- Model definition with declaration of tuning parameters (`workflow` and `pipeline`)
- Data split handling 
- Tuning and model selection

## Tidymodels recipes

```{r}
library(tidymodels) # For ML applications
library(timetk) 
split <- initial_time_split(
  data |>
    filter(industry == "manuf") |>
    select(-industry),
  prop = 4 / 5
)
```

- We start with a pre-processing plan (`recipe`)
- We remove the column *month*, include all interaction terms between factors and macroeconomic predictors, and demean and scale each variable such that the standard deviation is one
- Do not move on if it is unclear why we do not simply use `mutate`!

```{r}
rec <- recipe(ret_excess ~ ., data = training(split)) |>
  step_rm(month) |> # remove date variable
  step_interact(terms = ~ contains("factor"):contains("macro")) |> # interaction terms
  step_normalize(all_predictors()) |> # scale to unit standard deviation
  step_center(ret_excess , skip = TRUE) # demean variables
```

## Scikit-learn recipes

- Skicit-learn offers a similar range of preprocessing (`recipe`) steps
- Interaction terms have to be built manually

```{python}
#| eval: false
preprocessor = ColumnTransformer(
  transformers=[
    ("scale", StandardScaler(), 
    [col for col in data.columns 
      if col not in ["ret_excess", "month", "industry"]])
  ],
  remainder="drop",
  verbose_feature_names_out=False
)
```

## Build a model with tidymodels

- Makes use of a range of packages combined in `tidymodels`

```{r}
lm_model <- linear_reg(
  penalty = 0.0001,
  mixture = 1
) |> set_engine("glmnet", intercept = FALSE)
```

- `lm_model` contains the definition of our model with all required information. 
- `set_engine("glmnet")` indicates the API character of the `tidymodels` workflow: Under the hood, the package `glmnet` is doing the heavy lifting, while `linear_reg` provides a unified framework to collect the inputs
- Why is this amazing? You can change the model (e.g., change mixture to get Ridge or call neural net instead of linear regression)!

```{r}
lm_fit <- workflow() |>
  add_recipe(rec) |>
  add_model(lm_model)
```

- `workflow` ends with combining everything necessary for the (serious) data science workflow: a recipe and a model. So now we are ready to use `fit.`

```{r, eval = FALSE}
lm_fit |> fit(data = training(split))
```

## Build a model with Scikit-learn

- Similar to `tidymodels`: Define model and embed in `pipeline`
- `Fit` method to compute parameter estimates

```{python}
#| eval: false
lm_model = ElasticNet(
  alpha=0.007,
  l1_ratio=1, 
  max_iter=5000, 
  fit_intercept=False
)  

lm_pipeline = Pipeline([
  ("preprocessor", preprocessor),
  ("regressor", lm_model)
])


# Easy to fit a model
lm_fit = lm_pipeline.fit(
  data_manufacturing_training, 
  data_manufacturing_training.get("ret_excess")
)

```

## Tune a model with tidymodels
  
- Recall:  Specify a grid of hyperparameters, obtain predictors $\hat{y}_i(\lambda)$ to denote the predictors for the used parameters $\lambda$ and compute $\mbox{MSPE}(\lambda)$ 

```{r}
lm_model <- linear_reg(
  penalty = tune(),
  mixture = tune()
) |> set_engine("glmnet")
lm_fit <- lm_fit |> update_model(lm_model) # Update the existing model
```

- time-series cross-validation sample: tune with 20 random samples of length five years with a validation period of 4 years

```{r}
data_folds <- time_series_cv(
  data        = training(split),
  date_var    = month,
  initial     = "5 years",
  assess      = "48 months", cumulative  = FALSE, slice_limit = 20
)
```

```{r}
lm_tune <- lm_fit |>
  tune_grid(
    resample = data_folds,
    grid = grid_regular(penalty(), mixture(), levels = c(10, 3)),
    metrics = metric_set(rmse)
  )
```

## Select the best model 

```{r}
autoplot(lm_tune) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Cross-validated average squared prediction errors")
```

## Tune a model with Scikit-learn

```{python}
#| eval: false
data_folds = TimeSeriesSplit(
  n_splits=n_splits, 
  test_size=assessment_months, 
  max_train_size=initial_years * length_of_year
)

params = {
  "regressor__alpha": alphas,
  "regressor__l1_ratio": (0.0, 0.5, 1)
}

finder = GridSearchCV(
  lm_pipeline,
  param_grid=params,
  scoring="neg_root_mean_squared_error",
  cv=data_folds
)

finder = finder.fit(
  data_manufacturing, data_manufacturing.get("ret_excess")
)

```

## Advanced Tools

- In the exercises, you will go further: Compute lasso (with penalty as tuning parameter) for **all** industries in the sample
- The figure below illustrates for each industry the selected variables: What are your expectations? 

![]("figures/factor_selection_lasso.png")

# Nonlinear methods

## Regression Trees

- Regression trees have become a popular machine learning
approach for incorporating multiway predictor interactions
- Trees are fully nonparametric and possess a logic that departs markedly
from traditional regressions
- Trees are designed to find groups of observations that behave similarly 
- A tree "grows" in a sequence of steps 
- At each step, a new "branch" sorts the data leftover from the preceding step into bins based on one of the predictor variables
- This sequential branching slices the space of predictors into rectangular partitions and approximates the unknown function $f(x_i)$ with the average value of the outcome variable within each partition

## How do regression trees work?

- We partition the  predictor space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$
- For any predictor $x$ that falls within region $R_j$ we estimate $f(x)$ with the average of the training observations $y_i$ for which the associated predictor $x_i$ is also in $R_j$
- Once we select a partition $\mathbf{x}$ to split to create the new partitions, we find a predictor $j$ and value $s$ that define two new partitions, which we will call $R_1(j,s)$ and $R_2(j,s)$, that split our observations in the current partition by asking if $x_j$ is bigger than $s$:
$$
R_1(j,s) = \{\mathbf{x} \mid x_j < s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
$$
- To pick $j$ and $s$, we find the pair that minimizes the residual sum of square (RSS):
$$
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
$$
- Note: We do not scale by $\#R_k(j, s)$
- \textcolor{red}{Question: What are the hyperparameter decisions?}

## Intuition with R
- Also here: In the **exercises**, you will implement a regression tree on your own
```{r, out.width='70%', fig.align='center'}
library(rpart)
model <- rpart(
  ret_excess ~ ., 
  data = training(split) |> select(-month), 
  control = rpart.control(maxdepth = 2))

plot(model)
text(model, cex = 0.7)
```

## Random forests and Bagging
- Single tree models suffer from high variance 
- Pruning the tree helps reduce this variance
- Bootstrap aggregating (Bagging) is one such approach (initially proposed by Breiman, 1996)
- Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance. Bagging follows three simple steps:

### Growing a forest
- Build $B$ decision trees $T_1, \ldots, T_B$ using the training sample
- The bootstrap: Create a bootstrap training set by sampling $N$   observations from the training set with replacement
- Also: randomly selecting features to be included in the building of each tree
- For each observation in the test set, form a prediction $\hat{y} = \frac{1}{B}\sum\limits_{i=1}^B\hat{y}_{T_i}$

## Hyperparameter tuning for random forests (R)
```{r}
#| eval: false
# install.packages("ranger") # Regression trees in R
rf_model <- rand_forest(
  trees = 50, # tune()
  min_n = 20  # tune()
) |>
  set_engine("ranger") |>
  set_mode("regression")
```

- Fitting the model follows the same convention as for the penalized regressions before

```{r}
#| eval: false
rf_fit <- workflow() |>
  add_recipe(rec) |>
  add_model(rf_model) |>
  fit(data = training(split))
```

## Hyperparameter tuning for random forests (Python)
```{python}
#| eval: false
rf_model = RandomForestRegressor(
  n_estimators=50, 
  min_samples_leaf=20, 
  random_state=random_state
)
```

- Fitting the model follows the same convention as for the penalized regressions before

```{python}
#| eval: false
rf_pipeline = Pipeline([
  ("preprocessor", preprocessor),
  ("regressor", rf_model)
])
```

## Neural networks

### Three major takeaways from the biological neuron

1. The neuron only generates a signal if a sufficient number of input signals enter the neuron's dendrites (all or nothing)
2. Neurons receive inputs from many adjacent neurons upstream and can transmit signals to many adjacent signals downstream (cumulative inputs)
3. Each neuron has its threshold for activation (synaptic weight)

![]("figures/neural_network_one_unit.png"){width=70% fig-align="center"}

## Feed-forward neural networks

- Neural networks have theoretical underpinnings as "universal approximators" for
any smooth predictive association (Hornik, 1991)
- their complexity ranks neural networks among the *least transparent, least interpretable, and most
highly parameterized* machine learning tools
- consist of an "input layer" of raw predictors, one or more "hidden layers" that interact and nonlinearly transform the predictors, and an "output layer" that aggregates
hidden layers into an outcome prediction
- number of units (neurons) in the input layer is equal to the dimension of the predictors
- output layer usually consists of one neuron (for regression) or multiple neurons for classification

## Neural networks: Example

![]("figures/neural_network.jpg"){width=50% fig-align="center"}

- 4 input units
- One hidden layer with five neurons
- Each neuron receives information from each input layer
- The results from each neuron, $x_k^1 = f\left(\theta_k ^0 + \sum\limits_{j=1}^4 z_j\theta_{k,j}^0\right)$ are finally aggregated into one output forecast
$$\theta_0^1 + \sum\limits_{j=1}^5x_j^1\theta_j^1 $$


## Activation function

- Each neuron applies a nonlinear "activation function" $f$ to its aggregated signal before
sending its output to the next layer
$$x_k^l = f\left(\theta^k_{0} + \sum\limits_{j = 1}^{N ^l}z_j\theta_{l,j}^k\right)$$

- Easiest case: $f(x) = \alpha + \beta x$ resembles linear regression
- Typical activation functions are sigmoid ($f(x) = (1+e^{-x})^{-1}$) or ReLu ($f(x) = max(x, 0)$) 


```{r, echo = FALSE, size = "tiny", out.width='50%', fig.align='center'}
tab <- tibble(val = seq(-1.5, 1.5, 0.01)) |> 
  mutate(linear = 0 + val*0.2,
         ReLU = pmax(0, val),
         logistic = 1/(1 + exp(-1 -2*val)),
         tanh = tanh(val),
         binary = 1 * (val>0),
         softsign = val/(1+abs(val)))

tab |> pivot_longer(-val) |> 
  ggplot(aes(x = val, y = value)) +
  geom_line() +
  theme_minimal() +
  facet_wrap(~name) +
  labs(x = "", y = "") +
  geom_hline(aes(yintercept =0), linetype = "dotted")
```

## Neural networks: Architecture and Implementation

### Plenty of decisions 
- Depth (number of hidden layers), Activation function, number of neurons, connections of units (dense or sparse), regularization to avoid overfitting, learning rate
- There is no clear theoretical guidance on these choices but rather a large number of rules of thumbs 
- Despite the computational challenges, implementation in R and Python is not tedious at all
- Take a look [at this impressive visualization](https://playground.tensorflow.org/)
- Due to the data transformation process that Neural Networks perform, they are susceptible to the individual scale of the feature values: Standardize the feature sets!

## Backpropagation

- How to train the model?
- For given weights $w$, we run the neural network and receive output $\tilde y$ as a function of the input values $X$ (as well as the network architecture and the current weights)
- Compute the loss $\mathcal{L}\left(\tilde y, y\right)$, e.g. MSPE
- The aim is to choose $w$ to minimize the Loss
- $\Rightarrow$ Differentiate with respect to $w$ and move in the direction of the negative gradient (step size is a hyperparameter, often called learning rate) 

$$\Delta \vec{w} = r\cdot\left(\frac{\partial P}{\partial w_0},\frac{\partial P}{\partial w_1}, ... ,\frac{\partial P}{\partial w_q}\right)$$


- For a thorough discussion of backpropagation, consider watching [this lecture]()

## Deep neural networks in R

- `torch` provides a helpful interface to create and train a deep neural network
- The example below generates a sequential model with 21 input units, two hidden layers with 64 and 16 neurons, respectively, and ReLu activation functions
- Dropout and $L_2$ regularization is easy to include
- Output is a single neuron with linear (default) activation function which can be used for predictions

```{r, eval = FALSE}
library(torch)
library(brulee)

deep_nnet_model <- mlp(
  epochs = 500,
  dropout = 0.6,
  hidden_units = c(64, 16),
  activation = "sigmoid"
) |>
  set_mode("regression") |>
  set_engine("brulee", verbose = FALSE)
```


## Deep neural networks in Python

```{python}
#| eval: false

deepnnet_model = MLPRegressor(
  hidden_layer_sizes=(64, 16),
  activation="sigmoid", 
  solver="lbfgs",
  max_iter=500, 
  random_state=42 # seed
)
                              
deepnnet_pipeline = Pipeline([
  ("preprocessor", preprocessor),
  ("regressor", deepnnet_model)
])

```

## Empirical Asset Pricing via Machine Learning

- Now you are equipped with everything needed to follow Gu et al. (2020): Methods, Data, Procedure

### Hyperparameter tuning
- Consult the Online Appendix

![]("figures/gu_et_al_results_hyperparameter.jpg")

## Results 

- Portfolio sorts based on return prediction: "At the end of month $t$, we calculate one-month-ahead out-of-sample stock return predictions for each method. We then sort stocks into deciles based on each model's forecasts. We buy the highest expected return stocks (decile 10) and sell the lowest (decile 1). At the end of the month $t +  1$, we can calculate the realized returns of the portfolios (buy side and sell side
respectively)"

![]("figures/gu_et_al_results.jpg")

## Results 

- "for each method, we calculate the reduction in R2 from setting all values of a given predictor to zero within each training sample and average these into a single importance measure for each predictor. 
- "all methods agree on a relatively small set of dominant predictive signals, [...] associated with price trends including return reversal and momentum [...], stock liquidity, stock volatility, and valuation ratios.

![]("figures/gu_et_al_results_variable_importance.jpg"){width=50% fig-align="center"}

# Case studies

## Option Pricing

- Recall: The value of a call option for a non-dividend-paying underlying stock in terms of the Black–Scholes parameters is:
$$
\begin{aligned}
  C(S_t, t) &= N(d_1)S_t - N(d_2)Ke^{-r(T - t)} \\
     d_1 &= \frac{1}{\sigma\sqrt{T - t}}\left[\ln\left(\frac{S_t}{K}\right) + \left(r + \frac{\sigma^2}{2}\right)(T - t)\right] \\
     d_2 &= d_1 - \sigma\sqrt{T - t} \\
\end{aligned}
$$
- $V$ is the price of the option as a function of stock price $S$ and time $t$, $r$ is the risk-free interest rate, and $\sigma$ is the volatility of the stock. $N(\cdot)$ is the standard normal cumulative distribution function.

- Can machine learning methods **learn** the Black-Scholes equation after observing different specifications and corresponding prices?

### Start with simulated data

- We compute option prices for Call options for a grid of different combinations of maturity (`T`), risk-free rate (`r`), volatility (`sigma`), the strike price (`K`), and current stock price (`S`)
- To make it harder: Add an idiosyncratic error term to each observation

## "Learning" Black-Scholes

- Implementation left to you
- Considered models: Deep Neural Network, Lasso (based on a polynomial expansion of all inputs), Random Forests, and a single-layer neural network
- Below: Out-of-sample prediction error for different strike prices

![]("figures/option_pricing_case_study.png"){width=50% fig-align="center"}

## Image recognition

- Jiang et al. (2022) "train" a neural network on financial time series charts
- "reconsider the idea of trend-based predictability using methods that flexibly learn price patterns that are most predictive of future returns, rather than testing hypothesized or pre-specified patterns (e.g., momentum and reversal)."

![]("figures/price_charts.jpg"){width=50% fig-align="center"}

- Charts can be represented as vectors with length 
*horizon $\times$ resolution*
- Convolutional neural network: cross-parameter restrictions that dramatically reduce parameterization relative to standard feed-forward neural networks
- Replication code available on [tidy-finance.org](www.tidy-finance.org)

